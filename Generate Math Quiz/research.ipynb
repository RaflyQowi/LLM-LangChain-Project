{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'max_new_tokens': 256, 'temperature': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code Workspace\\Personal Project\\Personal Project - LangChain Project\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Calling llama model\n",
    "\n",
    "# llm = CTransformers(model=\"D:\\Code Workspace\\DL Model\\llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
    "#                     model_type = 'llama',\n",
    "#                     config = config)\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    model_kwargs = config\n",
    ")\n",
    "\n",
    "# llm = InferenceClient(\n",
    "#     \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Testing\\n\\nTesting is an important part of the development process. It ensures that the software is working as intended and helps to identify any bugs or issues that need to be fixed.\\n\\nThere are several types of testing that can be performed, including:\\n\\n* **Unit testing**: This involves testing individual units or components of the software to ensure that they are working correctly. Unit tests are typically written by developers and are automated, meaning that they can be run automatically as part of the build process.\\n* **Integration testing**: This involves testing how different units or components of the software work together. Integration tests are also typically automated and are run as part of the build process.\\n* **System testing**: This involves testing the software as a whole to ensure that it is functioning correctly. System tests are typically performed by a separate team of testers and may involve manual testing as well as automated tests.\\n* **Acceptance testing**: This involves testing the software to ensure that it meets the requirements and expectations of the users. Acceptance tests are typically performed by the users or customer and may involve manual testing.\\n\\nIt is important to perform testing throughout the development process, rather than waiting until the end. This allows bugs and issues'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create LLM Chaining\n",
    "questions_template = \"Generate a {selected_topic_level} math quiz on the topic of {selected_topic}. Include {num_quizzes} questions\"\n",
    "questions_prompt = PromptTemplate(input_variables=[\"selected_topic_level\", \"selected_topic\", \"num_quizzes\"],\n",
    "                                    template=questions_template)\n",
    "questions_chain = LLMChain(llm= llm,\n",
    "                            prompt = questions_prompt,\n",
    "                            output_key = \"questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_template = \"From this Question:\\n {questions}\\n\\n gave me answer to each one of them\"\n",
    "answer_prompt = PromptTemplate(input_variables = [\"questions\"],\n",
    "                                template = answer_template)\n",
    "answer_chain = LLMChain(llm = llm,\n",
    "                        prompt = answer_prompt,\n",
    "                        output_key = \"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Sequential Chaining\n",
    "seq_chain = SequentialChain(chains = [questions_chain, answer_chain],\n",
    "                            input_variables = ['selected_topic_level', 'selected_topic', 'num_quizzes'],\n",
    "                            # Here we return multiple variables\n",
    "                            output_variables = ['questions', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = seq_chain({'selected_topic_level': 'Elementary School Level', \n",
    "                            'selected_topic': 'Fraction', \n",
    "                            'num_quizzes' : 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'selected_topic_level': 'Elementary School Level',\n",
       " 'selected_topic': 'Fraction',\n",
       " 'num_quizzes': 3,\n",
       " 'questions': ' on the topic of Fraction.\\n\\n1. What is a fraction?\\na. A whole number\\nb. A part of a whole number\\nc. A number greater than 1\\n\\nAnswer: b. A part of a whole number\\n\\n2. What is the fraction of the shaded part in the figure below?\\n\\na. 1/4\\nb. 1/2\\nc. 3/4\\n\\nAnswer: b. 1/2\\n\\n3. If John ate 3/4 of a pizza, how much pizza did he eat?\\na. 1/4\\nb. 1/2\\nc. 3/4\\n\\nAnswer: c. 3/4',\n",
       " 'answer': '.\\n\\n1. What is a mixed number?\\na. A whole number\\nb. A part of a whole number\\nc. A number that has both a whole number and a fraction\\n\\nAnswer: c. A number that has both a whole number and a fraction\\n\\n2. What is the mixed number of the figure below?\\n\\na. 1 1/4\\nb. 1 1/2\\nc. 1 3/4\\n\\nAnswer: b. 1 1/2\\n\\n3. If John ate 1 1/2 pizzas, how much pizza did he eat in total?\\na. 1/2\\nb. 1 1/2\\nc. 3\\n\\nAnswer: c. 3\\n\\n4. What is an improper fraction?\\na. A fraction that is greater than 1\\nb. A fraction that is less than 1\\nc. A fraction that is equal to 1\\n\\nAnswer: a. A fraction that is greater than 1\\n\\n5. What is the improper fraction of the figure below?\\n\\na. 2/3\\nb. 3'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
